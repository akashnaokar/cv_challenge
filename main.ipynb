{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.cElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense,Conv2D , MaxPooling2D, Flatten,BatchNormalization,Dropout\n",
    "\n",
    "from sklearn.metrics import classification_report , confusion_matrix , accuracy_score , auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Unzipping data set to a local director  \"\"\"\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"F:\\\\Coding\\\\cv_challenge\\\\archive.zip\", 'r') as Dataset:\n",
    "    Dataset.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F:\\Coding\\cv_challenge\\NEU-DET\\train\\images\\crazing\n",
    "data_folder = \"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\"\n",
    "\n",
    "train_folder = os.path.join(data_folder, \"train\", \"images\", )\n",
    "train_images_subfolders = os.listdir(train_folder)\n",
    "train_image_files = []\n",
    "for subfolder in train_images_subfolders:\n",
    "    train_image_path = os.path.join(data_folder, \"train\", \"images\", subfolder)\n",
    "    train_image_files += os.listdir(train_image_path)\n",
    "    print(len(train_image_files))\n",
    "    # image = Image.open(train_image_path + \"\\\\\" + train_image_files[0])\n",
    "    # image.show()\n",
    "    print(train_image_path)\n",
    "\n",
    "train_anno_path = os.path.join(data_folder,\"train\", \"annotations\")\n",
    "train_anno_files = os.listdir(train_anno_path)\n",
    "# print(train_anno_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extracting Lables from XML files \"\"\"\n",
    "def parseXML(xmlfile):\n",
    "    # create element tree object \n",
    "    tree = et.parse(xmlfile)  \n",
    "    # get root element \n",
    "    root = tree.getroot()\n",
    "\n",
    "    for name in root.iter('name'):\n",
    "        label = (name.text)\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crazing', 'patches', 'inclusion', 'pitted_surface', 'rolled-in_scale', 'scratches']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get distinct class labels from xml files\n",
    "Returns a list of distinct labels\n",
    "\"\"\"\n",
    "\n",
    "labels = []\n",
    "for anno in train_anno_files:\n",
    "    label = parseXML(train_anno_path +\"\\\\\"+anno)\n",
    "    if label not in labels:\n",
    "        labels.append(label)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crazing': 0, 'patches': 1, 'inclusion': 2, 'pitted_surface': 3, 'rolled-in_scale': 4, 'scratches': 5}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Assigning distinct values to each distinct label\n",
    "\"\"\"\n",
    "image_labels = {name: index for index, name in enumerate(labels)}\n",
    "print(image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crazing_1\n",
      "crazing_10\n",
      "crazing_100\n",
      "crazing_101\n",
      "crazing_102\n",
      "crazing_103\n",
      "crazing_104\n",
      "crazing_105\n",
      "crazing_106\n",
      "crazing_107\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Makeing corresponding image-label pairs \"\"\"\n",
    "\n",
    "for img in train_image_files[:10]:\n",
    "    base_img, extn = img.split(\".\")\n",
    "    print(base_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"image/width\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 200\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"image/object/view\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"Unspecified\"\n",
      "      }\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Followed the article of Reading-Writing TFRecord for Images'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1 = \"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\\\\train\\\\images\\\\inclusion\\\\inclusion_1.jpg\"\n",
    "anno1 = \"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\\\\train\\\\annotations\\\\inclusion_1.xml\"\n",
    "image_string = open(i1, 'rb').read()\n",
    "label = None\n",
    "\n",
    "tree = et.parse(anno1)  \n",
    "root = tree.getroot()\n",
    "\n",
    "for name in image_labels:\n",
    "  if name in i1:\n",
    "    label =  image_labels[name]\n",
    "\n",
    "filename = root.find(\"filename\").text\n",
    "width = int(root.find(\"size/width\").text)\n",
    "height = int(root.find(\"size/height\").text)\n",
    "\n",
    "xmin = float(root.find(\"object/bndbox/xmin\").text)\n",
    "ymin = float(root.find(\"object/bndbox/ymin\").text)\n",
    "xmax = float(root.find(\"object/bndbox/xmax\").text)\n",
    "ymax = float(root.find(\"object/bndbox/ymax\").text)\n",
    "\n",
    "label_text = root.find(\"object/name\").text\n",
    "label = image_labels.get(label_text, -1)  # Get label index or -1 if not found\n",
    "    \n",
    "\n",
    "# Create a dictionary with features that may be relevant.\n",
    "def image_example(image_string, label):\n",
    "  image_shape = tf.io.decode_jpeg(image_string).shape\n",
    "  feature = {\n",
    "        \"image/width\": _int64_feature(width),\n",
    "        \"image/height\": _int64_feature(height),\n",
    "        \"image/filename\":_bytes_feature(bytes(filename, 'utf-8')),\n",
    "        # 'image/source_id': root.find(\"source\").text,\n",
    "        \"image/encodedrawdata\": _bytes_feature(image_string),\n",
    "        \"image/format\": _bytes_feature(bytes(filename.split(\".\")[-1], 'utf-8')),  # Assuming filename contains format\n",
    "        \"image/object/bbox/xmin\": _float_feature(xmin),\n",
    "        \"image/object/bbox/xmax\": _float_feature(xmax),\n",
    "        \"image/object/bbox/ymin\": _float_feature(ymin),\n",
    "        \"image/object/bbox/ymax\": _float_feature(ymax),\n",
    "        \"image/object/class/text\": _bytes_feature(bytes(label_text, 'utf-8')),\n",
    "        \"image/object/class/label\": _int64_feature(label),\n",
    "        \"image/object/class/single\": _int64_feature(label),  # Assuming single class per object\n",
    "        \"image/object/difficult\": _int64_feature(0),  # Difficult is 0\n",
    "        \"image/object/truncated\": _int64_feature(0),  # Truncated is 0\n",
    "        \"image/object/view\": _bytes_feature(bytes(\"Unspecified\", 'utf-8'))  # Assuming view is Unspecified for simplicity\n",
    "    }\n",
    "\n",
    "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "for line in str(image_example(image_string, label)).split('\\n')[:15]:\n",
    "  print(line)\n",
    "print('...')\n",
    "\n",
    "# print(str(image_example(image_string, label)))\n",
    "\"\"\"\n",
    "Followed the article of Reading-Writing TFRecord for Images\n",
    "https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fn(img):\n",
    "  feature = {\n",
    "        \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encodedrawdata': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/class/text': tf.io.VarLenFeature(tf.string),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/class/single': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/difficult': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/truncated': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/view': tf.io.VarLenFeature(tf.string)\n",
    "    }\n",
    "  tf_example = tf.train.Example(features=tf.train.Features(feature = feature))\n",
    "  return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Feature.CopyFrom() takes exactly one argument (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m record_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages.tfrecords\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# writer = tf.io.TFRecordWriter(output_path + record_file)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m   \u001b[38;5;66;03m# for example in examples:\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m tf_example \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mCoding\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mcv_challenge\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mNEU-DET\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mcrazing\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mcrazing_1.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# writer.write(tf_example.SerializeToString())\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# writer.close()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m, in \u001b[0;36mcreate_fn\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_fn\u001b[39m(img):\n\u001b[0;32m      2\u001b[0m   feature \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/width\u001b[39m\u001b[38;5;124m\"\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/height\u001b[39m\u001b[38;5;124m\"\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mFixedLenFeature([], tf\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage/object/view\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mVarLenFeature(tf\u001b[38;5;241m.\u001b[39mstring)\n\u001b[0;32m     19\u001b[0m     }\n\u001b[1;32m---> 20\u001b[0m   tf_example \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample(features\u001b[38;5;241m=\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf_example\n",
      "\u001b[1;31mTypeError\u001b[0m: Feature.CopyFrom() takes exactly one argument (3 given)"
     ]
    }
   ],
   "source": [
    "output_path = \"F:\\\\Coding\\\\cv_challenge\\\\TF-Data\\\\Tf_train\"\n",
    "record_file = 'images.tfrecords'\n",
    "# writer = tf.io.TFRecordWriter(output_path + record_file)\n",
    "\n",
    "  # for example in examples:\n",
    "tf_example = create_fn(\"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\\\\train\\\\images\\\\crazing\\\\crazing_1.jpg\")\n",
    "# writer.write(tf_example.SerializeToString())\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
