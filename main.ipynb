{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Coding\\cv_challenge\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense,Conv2D , MaxPooling2D, Flatten,BatchNormalization,Dropout\n",
    "\n",
    "from sklearn.metrics import classification_report , confusion_matrix , accuracy_score , auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import xml.etree.cElementTree as et\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Unzipping data set to a local director  \"\"\"\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"F:\\\\Coding\\\\cv_challenge\\\\archive.zip\", 'r') as Dataset:\n",
    "    Dataset.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 : Convert image data into TFRecordDataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = ('F:\\\\Coding\\\\cv_challenge\\\\NEU-DET')\n",
    "train_folder_path = os.path.join(data_folder, \"train\", )\n",
    "\n",
    "\n",
    "def create_img_list(folder_path):\n",
    "    image_list = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image_list.append(image_path)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "train_img_list = create_img_list(train_folder_path)\n",
    "# print(train_img_list)\n",
    "\n",
    "\n",
    "train_anno_path = os.path.join(data_folder,\"train\", \"annotations\")\n",
    "train_anno_files_list = os.listdir(train_anno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # F:\\Coding\\cv_challenge\\NEU-DET\\train\\images\\crazing\n",
    "# data_folder = \"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\"\n",
    "\n",
    "# train_folder_path = os.path.join(data_folder, \"train\", )\n",
    "# train_subfolders = os.listdir(train_folder_path)\n",
    "# train_images_subfolders_path = os.path.join(train_folder_path, \"images\",)\n",
    "# train_images_subfolders = os.listdir(train_images_subfolders_path)\n",
    "\n",
    "# train_image_file_names = []                                                              # Gives file names\n",
    "# for subfolder in train_images_subfolders:\n",
    "#     train_image_path = os.path.join(train_images_subfolders_path, subfolder)\n",
    "    \n",
    "#     print(train_image_path)\n",
    "#     train_image_file_names += os.listdir(train_image_path)\n",
    "#     # print((train_image_file_names))\n",
    "#     # image = Image.open(train_image_path + \"\\\\\" + train_image_files[0])\n",
    "#     # image.show()\n",
    "#     # print(train_image_path)\n",
    "\n",
    "# train_anno_path = os.path.join(data_folder,\"train\", \"annotations\")\n",
    "# train_anno_files_list = os.listdir(train_anno_path)\n",
    "# # print(train_anno_files_list)\n",
    "# print(train_image_file_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extracting Lables from XML files \"\"\"\n",
    "\n",
    "def parseXML(xmlfile):\n",
    "    # create element tree object \n",
    "    tree = et.parse(xmlfile)  \n",
    "    # get root element \n",
    "    root = tree.getroot()\n",
    "\n",
    "    data = {}\n",
    "    data['filename'] = root.find('filename').text\n",
    "\n",
    "    size = root.find('size')\n",
    "    data['size'] = {\n",
    "        'width': int(size.find('width').text),\n",
    "        'height': int(size.find('height').text),\n",
    "        'depth': int(size.find('depth').text)\n",
    "    }\n",
    "    \n",
    "    obj = root.find('object')\n",
    "    data['object'] = {\n",
    "        'name': obj.find('name').text,\n",
    "        'pose': obj.find('pose').text,\n",
    "        'truncated': int(obj.find('truncated').text),\n",
    "        'difficult': int(obj.find('difficult').text),\n",
    "        'bndbox': {\n",
    "            'xmin': int(obj.find('bndbox/xmin').text),\n",
    "            'ymin': int(obj.find('bndbox/ymin').text),\n",
    "            'xmax': int(obj.find('bndbox/xmax').text),\n",
    "            'ymax': int(obj.find('bndbox/ymax').text)\n",
    "        }\n",
    "    }\n",
    "    # objects = []\n",
    "    # for obj in root.findall('object'):\n",
    "    #     obj_data = {\n",
    "    #         'name': obj.find('name').text,\n",
    "    #         'pose': obj.find('pose').text,\n",
    "    #         'truncated': int(obj.find('truncated').text),\n",
    "    #         'difficult': int(obj.find('difficult').text),\n",
    "    #         'bndbox': {\n",
    "    #             'xmin': int(obj.find('bndbox/xmin').text),\n",
    "    #             'ymin': int(obj.find('bndbox/ymin').text),\n",
    "    #             'xmax': int(obj.find('bndbox/xmax').text),\n",
    "    #             'ymax': int(obj.find('bndbox/ymax').text)\n",
    "    #         }\n",
    "    #     }\n",
    "    #     objects.append(obj_data)\n",
    "\n",
    "    # data['objects'] = objects\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crazing', 'inclusion', 'patches', 'pitted_surface', 'rolled-in_scale', 'scratches']\n",
      "{'crazing': 0, 'inclusion': 1, 'patches': 2, 'pitted_surface': 3, 'rolled-in_scale': 4, 'scratches': 5}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get distinct class labels from xml files\n",
    "Returns a list of distinct labels\n",
    "\"\"\"\n",
    "\n",
    "labels = []\n",
    "for anno in train_anno_files_list:\n",
    "    attr = parseXML(train_anno_path +\"\\\\\"+anno)\n",
    "    label = attr[\"object\"][\"name\"]\n",
    "    if label not in labels:\n",
    "        labels.append(label)\n",
    "print(labels)\n",
    "\n",
    "\"\"\"\n",
    "Assigning distinct values to each distinct label\n",
    "\"\"\"\n",
    "image_labels = {name: index for index, name in enumerate(labels)}\n",
    "print(image_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i1 = \"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\\\\train\\\\images\\\\crazing\\\\crazing_1.jpg\"\n",
    "# anno1 = \"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\\\\train\\\\annotations\\\\crazing_1.xml\"\n",
    "# image_string = open(i1, 'rb').read()\n",
    "# label = None\n",
    "\n",
    "# tree = et.parse(anno1)  \n",
    "# root = tree.getroot()\n",
    "\n",
    "# for name in image_labels:\n",
    "#   if name in i1:\n",
    "#     label =  image_labels[name]\n",
    "\n",
    "# filename = root.find(\"filename\").text\n",
    "# width = int(root.find(\"size/width\").text)\n",
    "# height = int(root.find(\"size/height\").text)\n",
    "\n",
    "# xmin = float(root.find(\"object/bndbox/xmin\").text)\n",
    "# ymin = float(root.find(\"object/bndbox/ymin\").text)\n",
    "# xmax = float(root.find(\"object/bndbox/xmax\").text)\n",
    "# ymax = float(root.find(\"object/bndbox/ymax\").text)\n",
    "\n",
    "# label_text = root.find(\"object/name\").text\n",
    "# label = image_labels.get(label_text, -1)  # Get label index or -1 if not found\n",
    "    \n",
    "\n",
    "# # Create a dictionary with features that may be relevant.\n",
    "# def image_example(image_string, label):\n",
    "\n",
    "#   feature = {\n",
    "#         \"image/width\": _int64_feature(width),\n",
    "#         \"image/height\": _int64_feature(height),\n",
    "#         \"image/filename\":_bytes_feature(bytes(filename, 'utf-8')),\n",
    "#         # 'image/source_id': root.find(\"source\").text,\n",
    "#         \"image/encodedrawdata\": _bytes_feature(image_string),\n",
    "#         \"image/format\": _bytes_feature(bytes(filename.split(\".\")[-1], 'utf-8')),  # Assuming filename contains format\n",
    "#         \"image/object/bbox/xmin\": _float_feature(xmin),\n",
    "#         \"image/object/bbox/xmax\": _float_feature(xmax),\n",
    "#         \"image/object/bbox/ymin\": _float_feature(ymin),\n",
    "#         \"image/object/bbox/ymax\": _float_feature(ymax),\n",
    "#         \"image/object/class/text\": _bytes_feature(bytes(label_text, 'utf-8')),\n",
    "#         \"image/object/class/label\": _int64_feature(label),\n",
    "#         \"image/object/class/single\": _int64_feature(label),  # Assuming single class per object\n",
    "#         \"image/object/difficult\": _int64_feature(0),  # Difficult is 0\n",
    "#         \"image/object/truncated\": _int64_feature(0),  # Truncated is 0\n",
    "#         \"image/object/view\": _bytes_feature(bytes(\"Unspecified\", 'utf-8'))  # Assuming view is Unspecified for simplicity\n",
    "#     }\n",
    "\n",
    "#   return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "# for line in str(image_example(image_string, label)).split('\\n')[:15]:\n",
    "#   print(line)\n",
    "# print('...')\n",
    "\n",
    "# # print(str(image_example(image_string, label)))\n",
    "# \"\"\"\n",
    "# Followed the article of Reading-Writing TFRecord for Images\n",
    "# https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_example(*args):\n",
    "  \n",
    "  feature = {\n",
    "        \"image/width\": _int64_feature(width),\n",
    "        \"image/height\": _int64_feature(height),\n",
    "        \"image/filename\":_bytes_feature(bytes(filename, 'utf-8')),\n",
    "        # 'image/source_id': root.find(\"source\").text,\n",
    "        \"image/encodedrawdata\": _bytes_feature(image_string),\n",
    "        \"image/format\": _bytes_feature(bytes(filename.split(\".\")[-1], 'utf-8')),  # Assuming filename contains format\n",
    "        \"image/object/bbox/xmin\": _float_feature(xmin),\n",
    "        \"image/object/bbox/xmax\": _float_feature(xmax),\n",
    "        \"image/object/bbox/ymin\": _float_feature(ymin),\n",
    "        \"image/object/bbox/ymax\": _float_feature(ymax),\n",
    "        \"image/object/class/text\": _bytes_feature(bytes(label_text, 'utf-8')),\n",
    "        \"image/object/class/label\": _int64_feature(label),\n",
    "        \"image/object/class/single\": _int64_feature(label),  # Assuming single class per object\n",
    "        \"image/object/difficult\": _int64_feature(0),  # Difficult is 0\n",
    "        \"image/object/truncated\": _int64_feature(0),  # Truncated is 0\n",
    "        \"image/object/view\": _bytes_feature(bytes(\"Unspecified\", 'utf-8'))  # Assuming view is Unspecified for simplicity\n",
    "    }\n",
    "\n",
    "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "  \"\"\"\n",
    "  Followed the article of Reading-Writing TFRecord for Images\n",
    "  https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in train_img_list:\n",
    "  img_name_ext = img.split(\"\\\\\")[-1]\n",
    "  img_name = img_name_ext.split('.')[0]\n",
    "\n",
    "\n",
    "  for anno in train_anno_files_list:\n",
    "    anno_name = anno.split(\".\")[0]\n",
    "    \n",
    "    if img_name == anno_name:\n",
    "      attr = parseXML(train_anno_path +\"\\\\\"+anno)\n",
    "\n",
    "      image_string = open(img, 'rb').read()\n",
    "\n",
    "      filename = attr[\"filename\"]\n",
    "      # print(filename)\n",
    "      # print(filename.split('.')[1])\n",
    "      width = attr[\"size\"][\"width\"]\n",
    "      height = attr[\"size\"][\"height\"]\n",
    "\n",
    "      xmin = attr[\"object\"][\"bndbox\"][\"xmin\"]\n",
    "      ymin = attr[\"object\"][\"bndbox\"][\"ymin\"]\n",
    "      xmax = attr[\"object\"][\"bndbox\"][\"xmax\"]\n",
    "      ymax = attr[\"object\"][\"bndbox\"][\"ymax\"]\n",
    "\n",
    "      # for obj in attr[\"object\"]:\n",
    "      #   print(obj)\n",
    "      #   xmin = obj[\"bndbox\"][\"xmin\"]\n",
    "      #   ymin = obj[\"bndbox\"][\"ymin\"]\n",
    "      #   xmax = obj[\"bndbox\"][\"xmax\"]\n",
    "      #   ymax = obj[\"bndbox\"][\"ymax\"]\n",
    "\n",
    "      label_text = attr[\"object\"][\"name\"]\n",
    "      label = image_labels.get(label_text, -1)  # Get label index or -1 if not found\n",
    "\n",
    "      # for line in str(get_image_example(image_string, width, height, filename, xmin, xmax, ymin, ymax, label, label_text)).split('\\n')[:15]:\n",
    "      #   print(line)\n",
    "      # print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recordfile = 'F:\\\\Coding\\\\cv_challenge\\\\TF_Records'\n",
    "# with tf.io.TFRecordWriter(recordfile) as writer:\n",
    "#   # for filename, label in image_labels.items():\n",
    "#   image_string = open((\"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\\\\train\\\\images\\\\crazing\\\\crazing_1.jpg\"), \"rb\").read()\n",
    "#   tf_example = get_image_example(image_string, label)\n",
    "#   writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(image_paths, out_path):\n",
    "\n",
    "    for img in image_paths:\n",
    "        image_string  = open((img), 'rb').read()\n",
    "        tf_example = get_image_example(image_string, width, height, filename, xmin, xmax, ymin, ymax, label, label_text)\n",
    "\n",
    "        file_name_ext = os.path.basename(img)\n",
    "        file_name = os.path.splitext(file_name_ext)[0]\n",
    "        output_filename = os.path.join(out_path, f'{file_name}.tfrecord')\n",
    "        with tf.io.TFRecordWriter(output_filename) as writer:\n",
    "            writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(train_img_list, 'F:\\\\Coding\\\\cv_challenge\\\\TF_Records\\\\TFR_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"F:\\\\Coding\\\\cv_challenge\\\\TF-Data\\\\Tf_train\"\n",
    "# record_file = 'images.tfrecords'\n",
    "# # writer = tf.io.TFRecordWriter(output_path + record_file)\n",
    "\n",
    "#   # for example in examples:\n",
    "# tf_example = create_fn(\"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\\\\train\\\\images\\\\crazing\\\\crazing_1.jpg\")\n",
    "# # writer.write(tf_example.SerializeToString())\n",
    "\n",
    "# # writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fn(img):\n",
    "  feature = {\n",
    "        \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encodedrawdata': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/class/text': tf.io.VarLenFeature(tf.string),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/class/single': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/difficult': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/truncated': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/view': tf.io.VarLenFeature(tf.string)\n",
    "    }\n",
    "  tf_example = tf.train.Example(features=tf.train.Features(feature = feature))\n",
    "  return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"F:\\\\Coding\\\\cv_challenge\\\\TF-Data\\\\Tf_train\"\n",
    "record_file = 'images.tfrecords'\n",
    "# writer = tf.io.TFRecordWriter(output_path + record_file)\n",
    "\n",
    "  # for example in examples:\n",
    "tf_example = create_fn(\"F:\\\\Coding\\\\cv_challenge\\\\NEU-DET\\\\train\\\\images\\\\crazing\\\\crazing_1.jpg\")\n",
    "# writer.write(tf_example.SerializeToString())\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
